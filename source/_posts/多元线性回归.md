---
title: 多元线性回归
abbrlink: d21a
date: 2022-05-29 11:11:20
tags:
password:
---

## 多元线性回归模型



#### 什么是线性回归？

* 几何上，回归是寻找一条有代表性的直线或曲线(高维空之间的超平面)来拟合输入数据点和输出数据点。
* 线性回归可以理解为寻找用于拟合输入数据点和输出数据点的代表性直线或曲线的过程。



### 作用

* 主要进行预测



### 使用准则

* 自变量对[因变量](https://baike.baidu.com/item/因变量)必须有显著的影响，并呈密切的[线性相关](https://baike.baidu.com/item/线性相关)；
* 自变量与因变量之间的线性相关必须是真实的，而不是形式上的；
* 自变量应具有完整的[统计数据](https://baike.baidu.com/item/统计数据)，其预测值容易确定。
* [自变量](https://baike.baidu.com/item/自变量)之间应具有一定的互斥性，即自变量之间的相关程度不应高于自变量与因变量之间的相关程度；
* 



### 模型

![1](http://blog.axieyun.top/img/36.png)



### 显著性检验



#### *F*检验

![1](http://blog.axieyun.top/img/37.png)

* 用来检验整个回归模型的显著性；



#### *t*检验

![1](http://blog.axieyun.top/img/1.jpg)

* 在一定的显著性水平下，用来**检验** ==回归模型==中**各个自变量**是否具有显著性；
* 如果==*t*检验==的***p***值小于0.05，则该系数通过==*t*检验==，说明这个回归变量对因变量是显著的。





### 注意

* 当**线性回归模型**中有两个或者多个**自变量高度线性相关**时，使用**最小二乘法**建立回归模型有可能**失真 **，甚至会把分析引向歧途，这就是所谓的==多重共线性问题==。
* 如果模型存在严重的==多重共线性问题==，不能直接作为预测模型。
* 如果模型仅用于预测，则只要拟合程度好，可不处理多重共线性问题，存在多重共线性的模型用于预测时，往往不影响预测结果。
* 多元线性回归分析，需要作==多重共线性诊断==，以期得到较为合理的结果。
* 模型通过***F*检验**，说明整体回归模型是显著的，但不能说明回归模型中的每一个回归变量（自变量）都是显著的。





### 多重共线性诊断



#### 共线性的判别指标

* **方差膨胀因子**==VIF==：VIF值越大，多重共线性越严重。一般认为VIF大于10时（严格是5），代表模型存在严重的共线性问题。
* 如果VIF介于5~10之间视情况而定。
* 通常情况下，如果共线性情况不严重（VIF<5），不需要做特别的处理。
* 容差值：容差值=1/==VIF==，所以容差值大于0.1则说明没有共线性(严格是大于0.2)，VIF和容差值有逻辑对应关系，两个指标任选其一即可。
* 除此之外，直接对自变量进行相关分析，查看相关系数和显著性也是一种判断方法。
* * 如果一个自变量和其他自变量之间的相关系数显著，则代表可能存在多重共线性问题。





![1](http://blog.axieyun.top/img/2.jpg)

* 如果模型中大部分回归变量的容忍度均小于0.1，*==VIF==* 值大于10，表明回归变量之间存在==多重共线性问题==。
* 如果模型存在严重的==多重共线性问题==，不能直接作为预测模型。
* 适度的多重共线性不成问题，但当出现严重共线性问题时，会导致分析结果不稳定，出现回归系数的符号与实际情况完全相反的情况。
* 本应该显著的自变量不显著，本不显著的自变量却呈现出显著性，这种情况下就需要消除多重共线性的影响。



#### 共线性出现的原因

* 多重共线性问题就是指一个解释变量的变化引起另一个解释变量地变化。
* 原本自变量应该是各自独立的，根据回归分析结果，能得知哪些因素对因变量Y有显著影响，哪些没有影响。
* 如果各个自变量x之间有很强的线性关系，就无法固定其他变量，也就找不到x和y之间真实的关系了。
* 除此以外，多重共线性的原因还可能包括：
* * 错误地使用虚拟变量。（比如，同时将男、女两个虚拟变量都放入模型，此时必定出现共线性，称为完全共线性）
  * 数据不足。在某些情况下，收集更多数据可以解决共线性问题。





#### 有效解决办法



##### 1、手动移除出共线性的变量

~~~tex
先做下相关分析，如果发现某两个自变量X（解释变量）的相关系数值大于0.7，则移除掉一个自变量（解释变量），然后再做回归分析。此方法是最直接的方法，但有的时候我们不希望把某个自变量从模型中剔除，这样就要考虑使用其他方法。
~~~



##### 2、逐步回归法

~~~tex
让系统自动进行自变量的选择剔除，使用逐步回归将共线性的自变量自动剔除出去。此种解决办法有个问题是，可能算法会剔除掉本不想剔除的自变量，如果有此类情况产生，此时最好是使用岭回归进行分析。
~~~



##### 3、主成分回归

~~~tex
主成分分析法作为多元统计分析的一种常用方法在处理多变量问题时具有其一定的优越性，其降维的优势是明显的，主成分回归方法对于一般的多重共线性问题还是适用的，尤其是对共线性较强的变量之间。当采取主成分提取了新的变量后，往往这些变量间的组内差异小而组间差异大，起到了消除共线性的问题。
~~~



##### 4、增加样本容量

* 增加样本容量是解释共线性问题的一种办法，但在实际操作中可能并不太适合，原因是样本量的收集需要成本时间等。



##### 5、岭回归

* 第1和第2种解决办法在实际研究中使用较多，但问题在于，如果实际研究中并不想剔除掉某些自变量，某些自变量很重要，不能剔除。此时可能只有岭回归最为适合了。
* 岭回归是当前解决共线性问题最有效的解释办法。









### 参考

* [机器学习笔记之多重共线性问题以及如何解决 - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1796144)
* [(66条消息) 【超详细】多元线性回归模型statsmodels_ols_大闸謝Gemini的博客-CSDN博客_多元线性回归模型](https://blog.csdn.net/qq_42294351/article/details/119815102)



























